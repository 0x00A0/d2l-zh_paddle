
None None None # 线性回归的从零开始实现

.. _sec_linear_scratch:

在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。
在这一节中，我们将从零开始实现整个方法，
包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。
虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。
同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。
在这一节中，我们将只使用张量和自动求导。
在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。

.. code:: python

    %matplotlib inline
    import random
    import paddle
    from d2l import paddle as d2l

生成数据集
----------

为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。
我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。
我们将使用低维数据，这样可以很容易地将其可视化。
在下面的代码中，我们生成一个包含1000个样本的数据集，
每个样本包含从标准正态分布中采样的2个特征。
我们的合成数据集是一个矩阵\ :math:`\mathbf{X}\in \mathbb{R}^{1000 \times 2}`\ 。

我们使用线性模型参数\ :math:`\mathbf{w} = [2, -3.4]^\top`\ 、\ :math:`b = 4.2`
和噪声项\ :math:`\epsilon`\ 生成数据集及其标签：

.. math:: \mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.

你可以将\ :math:`\epsilon`\ 视为模型预测和标签时的潜在观测误差。
在这里我们认为标准假设成立，即\ :math:`\epsilon`\ 服从均值为0的正态分布。
为了简化问题，我们将标准差设为0.01。 下面的代码生成合成数据集。

.. code:: python

    def synthetic_data(w, b, num_examples):  #@save
        """生成y=Xw+b+噪声"""
        X = paddle.normal(0, 1, (num_examples, len(w)))
        y = paddle.matmul(X, w) + b
        y += paddle.normal(0, 0.01, y.shape)
        return X, y.reshape((-1, 1))

.. code:: python

    true_w = paddle.to_tensor([2, -3.4])
    true_b = 4.2
    features, labels = synthetic_data(true_w, true_b, 1000)

注意，\ ``features``\ 中的每一行都包含一个二维数据样本，
``labels``\ 中的每一行都包含一维标签值（一个标量）。

.. code:: python

    print('features:', features[0],'\nlabel:', labels[0])


.. parsed-literal::
    :class: output

    features: Tensor(shape=[2], dtype=float32, place=CPUPlace, stop_gradient=True,
           [0.36917415, 1.14585161]) 
    label: Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
           [1.04518020])


通过生成第二个特征\ ``features[:, 1]``\ 和\ ``labels``\ 的散点图，
可以直观观察到两者之间的线性关系。

.. code:: python

    d2l.set_figsize()
    d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1);


.. parsed-literal::
    :class: output

    D:\Anaconda3\envs\d2l\lib\site-packages\d2l\paddle.py:35: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`
      display.set_matplotlib_formats('svg')



.. figure:: output__linear-regression-scratch__be1878_8_1.svg


读取数据集
----------

回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。
由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，
该函数能打乱数据集中的样本并以小批量方式获取数据。

在下面的代码中，我们定义一个\ ``data_iter``\ 函数，
该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为\ ``batch_size``\ 的小批量。
每个小批量包含一组特征和标签。

.. code:: python

    def data_iter(batch_size, features, labels):
        num_examples = len(features)
        indices = list(range(num_examples))
        # 这些样本是随机读取的，没有特定的顺序
        random.shuffle(indices)
        for i in range(0, num_examples, batch_size):
            batch_indices = paddle.to_tensor(
                indices[i: min(i + batch_size, num_examples)])
            yield features[batch_indices], labels[batch_indices]

通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。
每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。
GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。

我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。
每个批量的特征维度显示批量大小和输入特征数。
同样的，批量的标签形状与\ ``batch_size``\ 相等。

.. code:: python

    batch_size = 10
    
    for X, y in data_iter(batch_size, features, labels):
        print(X, '\n', y)
        break


.. parsed-literal::
    :class: output

    Tensor(shape=[10, 2], dtype=float32, place=CPUPlace, stop_gradient=True,
           [[ 0.00760228,  0.21791586],
            [ 1.83002532, -0.09345292],
            [-0.17348005, -0.53726870],
            [-2.06337047,  0.09542230],
            [ 0.27952668,  1.11181331],
            [ 1.16761696, -0.74136072],
            [ 2.47060490, -0.27046424],
            [-0.61427116,  1.01177621],
            [-0.44147632,  0.20658316],
            [ 1.09019566, -0.22348033]]) 
     Tensor(shape=[10, 1], dtype=float32, place=CPUPlace, stop_gradient=True,
           [[ 3.46957517],
            [ 8.17458344],
            [ 5.67574835],
            [-0.26734629],
            [ 0.96260804],
            [ 9.05514240],
            [10.04859734],
            [-0.45727643],
            [ 2.62457132],
            [ 7.13361502]])


当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。
上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。
例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。
在深度学习框架中实现的内置迭代器效率要高得多，
它可以处理存储在文件中的数据和数据流提供的数据。

初始化模型参数
--------------

在我们开始用小批量随机梯度下降优化我们的模型参数之前，
我们需要先有一些参数。
在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，
并将偏置初始化为0。

.. code:: python

    w = paddle.normal(0., 0.01, shape=(2,1))
    b = paddle.zeros([1])

在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。
每次更新都需要计算损失函数关于模型参数的梯度。
有了这个梯度，我们就可以向减小损失的方向更新每个参数。
因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。 我们使用
:numref:`sec_autograd`\ 中引入的自动微分来计算梯度。

定义模型
--------

接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。
回想一下，要计算线性模型的输出，
我们只需计算输入特征\ :math:`\mathbf{X}`\ 和模型权重\ :math:`\mathbf{w}`\ 的矩阵-向量乘法后加上偏置\ :math:`b`\ 。
注意，上面的\ :math:`\mathbf{Xw}`\ 是一个向量，而\ :math:`b`\ 是一个标量。
回想一下 :numref:`subsec_broadcasting`\ 中描述的广播机制：
当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。

.. code:: python

    def linreg(X, w, b):  #@save
        """线性回归模型"""
        return paddle.matmul(X, w) + b

定义损失函数
------------

因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用
:numref:`sec_linear_regression`\ 中描述的平方损失函数。
在实现中，我们需要将真实值\ ``y``\ 的形状转换为和预测值\ ``y_hat``\ 的形状相同。

.. code:: python

    def squared_loss(y_hat, y):  #@save
        """均方损失"""
        return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

定义优化算法
------------

正如我们在
:numref:`sec_linear_regression`\ 中讨论的，线性回归有解析解。
尽管线性回归有解析解，但本书中的其他模型却没有。
这里我们介绍小批量随机梯度下降。

在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。
接下来，朝着减少损失的方向更新我们的参数。
下面的函数实现小批量随机梯度下降更新。
该函数接受模型参数集合、学习速率和批量大小作为输入。每
一步更新的大小由学习速率\ ``lr``\ 决定。
因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（\ ``batch_size``\ ）
来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。

.. code:: python

    def sgd(params, lr, batch_size):  #@save
        """小批量随机梯度下降"""
        with paddle.no_grad():  # 由于Paddle框架的问题,即使在no_grad下也必须手动修改stop_gradient来控制带梯度参数的inplace操作,该bug已提交Issue:https://github.com/PaddlePaddle/Paddle/issues/38016
            for param in params:
                param.stop_gradient=True
                param.subtract_(lr * param.grad / batch_size)
                param.clear_grad()

训练
----

现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。
理解这段代码至关重要，因为从事深度学习后，
你会一遍又一遍地看到几乎相同的训练过程。
在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。
计算完损失后，我们开始反向传播，存储每个参数的梯度。
最后，我们调用优化算法\ ``sgd``\ 来更新模型参数。

概括一下，我们将执行以下循环：

-  初始化参数
-  重复以下训练，直到完成

   -  计算梯度\ :math:`\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)`
   -  更新参数\ :math:`(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}`

在每个\ *迭代周期*\ （epoch）中，我们使用\ ``data_iter``\ 函数遍历整个数据集，
并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。
这里的迭代周期个数\ ``num_epochs``\ 和学习率\ ``lr``\ 都是超参数，分别设为3和0.03。
设置超参数很棘手，需要通过反复试验进行调整。
我们现在忽略这些细节，以后会在
:numref:`chap_optimization`\ 中详细介绍。

.. code:: python

    lr = 0.03
    num_epochs = 3
    net = linreg
    loss = squared_loss

.. code:: python

    for epoch in range(num_epochs):
        for X, y in data_iter(batch_size, features, labels):
            w.stop_gradient=False
            b.stop_gradient=False
            l = loss(net(X, w, b), y)  # X和y的小批量损失
            # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
            # 并以此计算关于[w,b]的梯度
            l.sum().backward()
            sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
        with paddle.no_grad():
            train_l = loss(net(features, w, b), labels)
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')


.. parsed-literal::
    :class: output

    D:\Anaconda3\envs\d2l\lib\site-packages\paddle\fluid\dygraph\varbase_patch_methods.py:392: UserWarning: 
    Warning:
    tensor.grad will return the tensor value of the gradient. This is an incompatible upgrade for tensor.grad API.  It's return type changes from numpy.ndarray in version 2.0 to paddle.Tensor in version 2.1.0.  If you want to get the numpy value of the gradient, you can use :code:`x.grad.numpy()` 
      warnings.warn(warning_msg)
    epoch 1, loss 0.033006
    epoch 2, loss 0.000120
    epoch 3, loss 0.000052


因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。
因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。
事实上，真实参数和通过训练学到的参数确实非常接近。

.. code:: python

    print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
    print(f'b的估计误差: {true_b - b}')


.. parsed-literal::
    :class: output

    w的估计误差: Tensor(shape=[2], dtype=float32, place=CPUPlace, stop_gradient=True,
           [ 0.00005352, -0.00022674])
    b的估计误差: Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
           [0.00006580])


注意，我们不应该想当然地认为我们能够完美地求解参数。
在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。
幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。
其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。

小结
----

-  我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。
-  这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。

练习
----

1. 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？
2. 假设你是\ `乔治·西蒙·欧姆 <https://en.wikipedia.org/wiki/Georg_Ohm>`__\ ，试图为电压和电流的关系建立一个模型。你能使用自动微分来学习模型的参数吗?
3. 您能基于\ `普朗克定律 <https://en.wikipedia.org/wiki/Planck%27s_law>`__\ 使用光谱能量密度来确定物体的温度吗？
4. 如果你想计算二阶导数可能会遇到什么问题？你会如何解决这些问题？
5. 为什么在\ ``squared_loss``\ 函数中需要使用\ ``reshape``\ 函数？
6. 尝试使用不同的学习率，观察损失函数值下降的快慢。
7. 如果样本个数不能被批量大小整除，\ ``data_iter``\ 函数的行为会有什么变化？
