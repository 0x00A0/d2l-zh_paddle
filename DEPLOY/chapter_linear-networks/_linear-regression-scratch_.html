<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>生成数据集 &#8212; 动手学深度学习 2.0.0-beta0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">生成数据集</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_linear-networks/_linear-regression-scratch_.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-pytorch.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-paddle.pdf">
                  <i class="fas fa-file-pdf"></i>
                  Paddle
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.zip">
                  <i class="fas fa-download"></i>
                  Jupyter 记事本
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai/zh-v2/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/0x00A0/d2l-zh_paddle">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <p>None None None # 线性回归的从零开始实现</p>
<p id="sec-linear-scratch">在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。
在这一节中，我们将从零开始实现整个方法，
包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。
虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。
同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。
在这一节中，我们将只使用张量和自动求导。
在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">paddle</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">paddle</span> <span class="k">as</span> <span class="n">d2l</span>
</pre></div>
</div>
<div class="section" id="id1">
<h1>生成数据集<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。
我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。
我们将使用低维数据，这样可以很容易地将其可视化。
在下面的代码中，我们生成一个包含1000个样本的数据集，
每个样本包含从标准正态分布中采样的2个特征。
我们的合成数据集是一个矩阵<span class="math notranslate nohighlight">\(\mathbf{X}\in \mathbb{R}^{1000 \times 2}\)</span>。</p>
<p>我们使用线性模型参数<span class="math notranslate nohighlight">\(\mathbf{w} = [2, -3.4]^\top\)</span>、<span class="math notranslate nohighlight">\(b = 4.2\)</span>
和噪声项<span class="math notranslate nohighlight">\(\epsilon\)</span>生成数据集及其标签：</p>
<div class="math notranslate nohighlight" id="equation-chapter-linear-networks-linear-regression-scratch-0">
<span class="eqno">()<a class="headerlink" href="#equation-chapter-linear-networks-linear-regression-scratch-0" title="Permalink to this equation">¶</a></span>\[\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.\]</div>
<p>你可以将<span class="math notranslate nohighlight">\(\epsilon\)</span>视为模型预测和标签时的潜在观测误差。
在这里我们认为标准假设成立，即<span class="math notranslate nohighlight">\(\epsilon\)</span>服从均值为0的正态分布。
为了简化问题，我们将标准差设为0.01。 下面的代码生成合成数据集。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">synthetic_data</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;生成y=Xw+b+噪声&quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">num_examples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">y</span> <span class="o">+=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">true_w</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4</span><span class="p">])</span>
<span class="n">true_b</span> <span class="o">=</span> <span class="mf">4.2</span>
<span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">synthetic_data</span><span class="p">(</span><span class="n">true_w</span><span class="p">,</span> <span class="n">true_b</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>注意，<code class="docutils literal notranslate"><span class="pre">features</span></code>中的每一行都包含一个二维数据样本，
<code class="docutils literal notranslate"><span class="pre">labels</span></code>中的每一行都包含一维标签值（一个标量）。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;features:&#39;</span><span class="p">,</span> <span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">label:&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">features</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[</span><span class="mf">0.36917415</span><span class="p">,</span> <span class="mf">1.14585161</span><span class="p">])</span>
<span class="n">label</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[</span><span class="mf">1.04518020</span><span class="p">])</span>
</pre></div>
</div>
<p>通过生成第二个特征<code class="docutils literal notranslate"><span class="pre">features[:,</span> <span class="pre">1]</span></code>和<code class="docutils literal notranslate"><span class="pre">labels</span></code>的散点图，
可以直观观察到两者之间的线性关系。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">features</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
<pre class="output literal-block">D:Anaconda3envsd2llibsite-packagesd2lpaddle.py:35: DeprecationWarning: <cite>set_matplotlib_formats</cite> is deprecated since IPython 7.23, directly use <cite>matplotlib_inline.backend_inline.set_matplotlib_formats()</cite>
  display.set_matplotlib_formats('svg')</pre>
<div class="figure align-default">
<img alt="../_images/output__linear-regression-scratch__be1878_8_1.svg" src="../_images/output__linear-regression-scratch__be1878_8_1.svg" /></div>
</div>
<div class="section" id="id2">
<h1>读取数据集<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h1>
<p>回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。
由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，
该函数能打乱数据集中的样本并以小批量方式获取数据。</p>
<p>在下面的代码中，我们定义一个<code class="docutils literal notranslate"><span class="pre">data_iter</span></code>函数，
该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>的小批量。
每个小批量包含一组特征和标签。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">data_iter</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">))</span>
    <span class="c1"># 这些样本是随机读取的，没有特定的顺序</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">features</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
</pre></div>
</div>
<p>通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。
每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。
GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。</p>
<p>我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。
每个批量的特征维度显示批量大小和输入特征数。
同样的，批量的标签形状与<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>相等。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[[</span> <span class="mf">0.00760228</span><span class="p">,</span>  <span class="mf">0.21791586</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.83002532</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.09345292</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.17348005</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.53726870</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">2.06337047</span><span class="p">,</span>  <span class="mf">0.09542230</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.27952668</span><span class="p">,</span>  <span class="mf">1.11181331</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.16761696</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.74136072</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.47060490</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27046424</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.61427116</span><span class="p">,</span>  <span class="mf">1.01177621</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.44147632</span><span class="p">,</span>  <span class="mf">0.20658316</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.09019566</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.22348033</span><span class="p">]])</span>
 <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[[</span> <span class="mf">3.46957517</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.17458344</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">5.67574835</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.26734629</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.96260804</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">9.05514240</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">10.04859734</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.45727643</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.62457132</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">7.13361502</span><span class="p">]])</span>
</pre></div>
</div>
<p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。
上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。
例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。
在深度学习框架中实现的内置迭代器效率要高得多，
它可以处理存储在文件中的数据和数据流提供的数据。</p>
</div>
<div class="section" id="id3">
<h1>初始化模型参数<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h1>
<p>在我们开始用小批量随机梯度下降优化我们的模型参数之前，
我们需要先有一些参数。
在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，
并将偏置初始化为0。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。
每次更新都需要计算损失函数关于模型参数的梯度。
有了这个梯度，我们就可以向减小损失的方向更新每个参数。
因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。 我们使用
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_autograd</span></code>中引入的自动微分来计算梯度。</p>
</div>
<div class="section" id="id4">
<h1>定义模型<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h1>
<p>接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。
回想一下，要计算线性模型的输出，
我们只需计算输入特征<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>和模型权重<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>的矩阵-向量乘法后加上偏置<span class="math notranslate nohighlight">\(b\)</span>。
注意，上面的<span class="math notranslate nohighlight">\(\mathbf{Xw}\)</span>是一个向量，而<span class="math notranslate nohighlight">\(b\)</span>是一个标量。
回想一下 <a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting"><span class="std std-numref">2.3节</span></a>中描述的广播机制：
当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;线性回归模型&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h1>定义损失函数<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h1>
<p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_regression</span></code>中描述的平方损失函数。
在实现中，我们需要将真实值<code class="docutils literal notranslate"><span class="pre">y</span></code>的形状转换为和预测值<code class="docutils literal notranslate"><span class="pre">y_hat</span></code>的形状相同。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">squared_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h1>定义优化算法<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h1>
<p>正如我们在
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_regression</span></code>中讨论的，线性回归有解析解。
尽管线性回归有解析解，但本书中的其他模型却没有。
这里我们介绍小批量随机梯度下降。</p>
<p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。
接下来，朝着减少损失的方向更新我们的参数。
下面的函数实现小批量随机梯度下降更新。
该函数接受模型参数集合、学习速率和批量大小作为输入。每
一步更新的大小由学习速率<code class="docutils literal notranslate"><span class="pre">lr</span></code>决定。
因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>）
来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">paddle</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># 由于Paddle框架的问题,即使在no_grad下也必须手动修改stop_gradient来控制带梯度参数的inplace操作,该bug已提交Issue:https://github.com/PaddlePaddle/Paddle/issues/38016</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span>
            <span class="n">param</span><span class="o">.</span><span class="n">subtract_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">param</span><span class="o">.</span><span class="n">clear_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h1>训练<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h1>
<p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。
理解这段代码至关重要，因为从事深度学习后，
你会一遍又一遍地看到几乎相同的训练过程。
在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。
计算完损失后，我们开始反向传播，存储每个参数的梯度。
最后，我们调用优化算法<code class="docutils literal notranslate"><span class="pre">sgd</span></code>来更新模型参数。</p>
<p>概括一下，我们将执行以下循环：</p>
<ul class="simple">
<li><p>初始化参数</p></li>
<li><p>重复以下训练，直到完成</p>
<ul>
<li><p>计算梯度<span class="math notranslate nohighlight">\(\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)\)</span></p></li>
<li><p>更新参数<span class="math notranslate nohighlight">\((\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}\)</span></p></li>
</ul>
</li>
</ul>
<p>在每个<em>迭代周期</em>（epoch）中，我们使用<code class="docutils literal notranslate"><span class="pre">data_iter</span></code>函数遍历整个数据集，
并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。
这里的迭代周期个数<code class="docutils literal notranslate"><span class="pre">num_epochs</span></code>和学习率<code class="docutils literal notranslate"><span class="pre">lr</span></code>都是超参数，分别设为3和0.03。
设置超参数很棘手，需要通过反复试验进行调整。
我们现在忽略这些细节，以后会在
<code class="xref std std-numref docutils literal notranslate"><span class="pre">chap_optimization</span></code>中详细介绍。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">linreg</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">squared_loss</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">w</span><span class="o">.</span><span class="n">stop_gradient</span><span class="o">=</span><span class="kc">False</span>
        <span class="n">b</span><span class="o">.</span><span class="n">stop_gradient</span><span class="o">=</span><span class="kc">False</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># X和y的小批量损失</span>
        <span class="c1"># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span>
        <span class="c1"># 并以此计算关于[w,b]的梯度</span>
        <span class="n">l</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">sgd</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># 使用参数的梯度更新参数</span>
    <span class="k">with</span> <span class="n">paddle</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, loss </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">train_l</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<pre class="output literal-block">D:Anaconda3envsd2llibsite-packagespaddlefluiddygraphvarbase_patch_methods.py:392: UserWarning:
Warning:
tensor.grad will return the tensor value of the gradient. This is an incompatible upgrade for tensor.grad API.  It's return type changes from numpy.ndarray in version 2.0 to paddle.Tensor in version 2.1.0.  If you want to get the numpy value of the gradient, you can use <code class="code docutils literal notranslate"><span class="pre">x.grad.numpy()</span></code>
  warnings.warn(warning_msg)
epoch 1, loss 0.033006
epoch 2, loss 0.000120
epoch 3, loss 0.000052</pre>
<p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。
因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。
事实上，真实参数和通过训练学到的参数确实非常接近。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;w的估计误差: </span><span class="si">{</span><span class="n">true_w</span> <span class="o">-</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;b的估计误差: </span><span class="si">{</span><span class="n">true_b</span> <span class="o">-</span> <span class="n">b</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w的估计误差</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[</span> <span class="mf">0.00005352</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00022674</span><span class="p">])</span>
<span class="n">b的估计误差</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="p">[</span><span class="mf">0.00006580</span><span class="p">])</span>
</pre></div>
</div>
<p>注意，我们不应该想当然地认为我们能够完美地求解参数。
在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。
幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。
其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。</p>
</div>
<div class="section" id="id8">
<h1>小结<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。</p></li>
<li><p>这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。</p></li>
</ul>
</div>
<div class="section" id="id9">
<h1>练习<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h1>
<ol class="arabic simple">
<li><p>如果我们将权重初始化为零，会发生什么。算法仍然有效吗？</p></li>
<li><p>假设你是<a class="reference external" href="https://en.wikipedia.org/wiki/Georg_Ohm">乔治·西蒙·欧姆</a>，试图为电压和电流的关系建立一个模型。你能使用自动微分来学习模型的参数吗?</p></li>
<li><p>您能基于<a class="reference external" href="https://en.wikipedia.org/wiki/Planck%27s_law">普朗克定律</a>使用光谱能量密度来确定物体的温度吗？</p></li>
<li><p>如果你想计算二阶导数可能会遇到什么问题？你会如何解决这些问题？</p></li>
<li><p>为什么在<code class="docutils literal notranslate"><span class="pre">squared_loss</span></code>函数中需要使用<code class="docutils literal notranslate"><span class="pre">reshape</span></code>函数？</p></li>
<li><p>尝试使用不同的学习率，观察损失函数值下降的快慢。</p></li>
<li><p>如果样本个数不能被批量大小整除，<code class="docutils literal notranslate"><span class="pre">data_iter</span></code>函数的行为会有什么变化？</p></li>
</ol>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">生成数据集</a></li>
<li><a class="reference internal" href="#id2">读取数据集</a></li>
<li><a class="reference internal" href="#id3">初始化模型参数</a></li>
<li><a class="reference internal" href="#id4">定义模型</a></li>
<li><a class="reference internal" href="#id5">定义损失函数</a></li>
<li><a class="reference internal" href="#id6">定义优化算法</a></li>
<li><a class="reference internal" href="#id7">训练</a></li>
<li><a class="reference internal" href="#id8">小结</a></li>
<li><a class="reference internal" href="#id9">练习</a></li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        
        </main>
    </div>
  </body>
</html>