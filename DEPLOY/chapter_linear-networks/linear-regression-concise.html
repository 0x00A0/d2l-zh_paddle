<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>3.7. 生成数据集 &#8212; 动手学深度学习 2.0.0-beta0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.16. softmax回归" href="softmax-regression.html" />
    <link rel="prev" title="3.1. 线性回归的基本元素" href="linear-regression.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">3. </span>线性神经网络</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">3.7. </span>生成数据集</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_linear-networks/linear-regression-concise.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-pytorch.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-paddle.pdf">
                  <i class="fas fa-file-pdf"></i>
                  Paddle
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.zip">
                  <i class="fas fa-download"></i>
                  Jupyter 记事本
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai/zh-v2/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/0x00A0/d2l-zh_paddle">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-concise.html#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <p>None None # 线性回归的简洁实现</p>
<p id="sec-linear-concise">在过去的几年里，出于对深度学习强烈的兴趣，
许多公司、学者和业余爱好者开发了各种成熟的开源框架。
这些框架可以自动化基于梯度的学习算法中重复性的工作。 在
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>中，我们只运用了：
（1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。
实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用，
现代深度学习库也为我们实现了这些组件。</p>
<p>在本节中，我们将介绍如何通过使用深度学习框架来简洁地实现
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>中的线性回归模型。</p>
<div class="section" id="id1">
<h1><span class="section-number">3.7. </span>生成数据集<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>与 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>中类似，我们首先生成数据集。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle</span>
<span class="kn">from</span> <span class="nn">paddle</span> <span class="kn">import</span> <span class="n">io</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">paddle</span> <span class="k">as</span> <span class="n">d2l</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Matplotlib</span> <span class="n">created</span> <span class="n">a</span> <span class="n">temporary</span> <span class="n">config</span><span class="o">/</span><span class="n">cache</span> <span class="n">directory</span> <span class="n">at</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">matplotlib</span><span class="o">-</span><span class="n">hwoi_8xl</span> <span class="n">because</span> <span class="n">the</span> <span class="n">default</span> <span class="n">path</span> <span class="p">(</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wan</span><span class="o">/.</span><span class="n">config</span><span class="o">/</span><span class="n">matplotlib</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">a</span> <span class="n">writable</span> <span class="n">directory</span><span class="p">;</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">highly</span> <span class="n">recommended</span> <span class="n">to</span> <span class="nb">set</span> <span class="n">the</span> <span class="n">MPLCONFIGDIR</span> <span class="n">environment</span> <span class="n">variable</span> <span class="n">to</span> <span class="n">a</span> <span class="n">writable</span> <span class="n">directory</span><span class="p">,</span> <span class="ow">in</span> <span class="n">particular</span> <span class="n">to</span> <span class="n">speed</span> <span class="n">up</span> <span class="n">the</span> <span class="kn">import</span> <span class="nn">of</span> <span class="n">Matplotlib</span> <span class="ow">and</span> <span class="n">to</span> <span class="n">better</span> <span class="n">support</span> <span class="n">multiprocessing</span><span class="o">.</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">true_w</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">paddle</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4</span><span class="p">])</span>
<span class="n">true_b</span> <span class="o">=</span> <span class="mf">4.2</span>
<span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">synthetic_data</span><span class="p">(</span><span class="n">true_w</span><span class="p">,</span> <span class="n">true_b</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h1><span class="section-number">3.8. </span>读取数据集<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h1>
<p>我们可以调用框架中现有的API来读取数据。
我们将<code class="docutils literal notranslate"><span class="pre">features</span></code>和<code class="docutils literal notranslate"><span class="pre">labels</span></code>作为API的参数传递，并通过数据迭代器指定<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>。
此外，布尔值<code class="docutils literal notranslate"><span class="pre">is_train</span></code>表示是否希望数据迭代器对象在每个迭代周期内打乱数据。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_array</span><span class="p">(</span><span class="n">data_arrays</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>  <span class="c1">#@save</span>
    <span class="sd">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">data_arrays</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">io</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">is_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">data_iter</span> <span class="o">=</span> <span class="n">load_array</span><span class="p">((</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<p>使用<code class="docutils literal notranslate"><span class="pre">data_iter</span></code>的方式与我们在
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>中使用<code class="docutils literal notranslate"><span class="pre">data_iter</span></code>函数的方式相同。为了验证是否正常工作，让我们读取并打印第一个小批量样本。
与
<code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>不同，这里我们使用<code class="docutils literal notranslate"><span class="pre">iter</span></code>构造Python迭代器，并使用<code class="docutils literal notranslate"><span class="pre">next</span></code>从迭代器中获取第一项。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_iter</span><span class="p">))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">[[</span> <span class="mf">0.26695454</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.61031514</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.56811917</span><span class="p">,</span>  <span class="mf">0.23991548</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.48661160</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14512579</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.28009248</span><span class="p">,</span>  <span class="mf">0.89850241</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.56320649</span><span class="p">,</span>  <span class="mf">0.75310522</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.72914153</span><span class="p">,</span>  <span class="mf">1.73198748</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.80055797</span><span class="p">,</span>  <span class="mf">0.45330566</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.25546420</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.38003913</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.00679264</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.04314697</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.06246236</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14536108</span><span class="p">]]),</span>
 <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="n">CPUPlace</span><span class="p">,</span> <span class="n">stop_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">[[</span><span class="mf">6.81621885</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">6.53260994</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">1.71733129</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">1.72119415</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">0.51202148</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.23238815</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">6.24916029</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">2.98804927</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">7.74188757</span><span class="p">],</span>
         <span class="p">[</span><span class="mf">4.59819794</span><span class="p">]])]</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h1><span class="section-number">3.9. </span>定义模型<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h1>
<p>当我们在 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_scratch</span></code>中实现线性回归时，
我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。
但是，如果模型变得更加复杂，且当你几乎每天都需要实现模型时，你会想简化这个过程。
这种情况类似于为自己的博客从零开始编写网页。
做一两次是有益的，但如果每个新博客你就花一个月的时间重新开始编写网页，那并不高效。</p>
<p>对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。
我们首先定义一个模型变量<code class="docutils literal notranslate"><span class="pre">net</span></code>，它是一个<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>类的实例。
<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>类将多个层串联在一起。
当给定输入数据时，<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>实例将数据传入到第一层，
然后将第一层的输出作为第二层的输入，以此类推。
在下面的例子中，我们的模型只包含一个层，因此实际上不需要<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>。
但是由于以后几乎所有的模型都是多层的，在这里使用<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>会让你熟悉“标准的流水线”。</p>
<p>回顾 <a class="reference internal" href="linear-regression.html#fig-single-neuron"><span class="std std-numref">图3.4.1</span></a>中的单层网络架构，
这一单层被称为<em>全连接层</em>（fully-connected layer），
因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。</p>
<p>在Paddle中，全连接层在<code class="docutils literal notranslate"><span class="pre">Linear</span></code>类中定义。
值得注意的是，我们将两个参数传递到<code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>中。
第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># nn是神经网络的缩写</span>
<span class="kn">from</span> <span class="nn">paddle</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h1><span class="section-number">3.10. </span>初始化模型参数<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h1>
<p>在使用<code class="docutils literal notranslate"><span class="pre">net</span></code>之前，我们需要初始化模型参数。
如在线性回归模型中的权重和偏置。
深度学习框架通常有预定义的方法来初始化参数。
在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，
偏置参数将初始化为零。</p>
<p>paddle<code class="docutils literal notranslate"><span class="pre">initializers</span></code>模块提供了多种模型参数初始化方法。
在paddle中最简单的指定初始化方法是在创建层时指定参数,参数由<code class="docutils literal notranslate"><span class="pre">ParamAttr</span></code>方法生成
在这里，我们通过<code class="docutils literal notranslate"><span class="pre">net[0]</span></code>选择网络中的第一个图层，然后重新创建了它</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">weight_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ParamAttr</span><span class="p">(</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="n">bias_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ParamAttr</span><span class="p">(</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">weight_attr</span><span class="o">=</span><span class="n">weight_attr</span><span class="p">,</span> <span class="n">bias_attr</span><span class="o">=</span><span class="n">bias_attr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h1><span class="section-number">3.11. </span>定义损失函数<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h1>
<p>计算均方误差使用的是<code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>类，也称为平方<span class="math notranslate nohighlight">\(L_2\)</span>范数。
默认情况下，它返回所有样本损失的平均值。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h1><span class="section-number">3.12. </span>定义优化算法<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h1>
<p>小批量随机梯度下降算法是一种优化神经网络的标准工具，
Paddle在<code class="docutils literal notranslate"><span class="pre">optimizer</span></code>模块中实现了该算法的许多变种。
动态图模式下,当我们实例化一个<code class="docutils literal notranslate"><span class="pre">SGD</span></code>实例时，我们要指定优化的参数
（可通过<code class="docutils literal notranslate"><span class="pre">net.parameters()</span></code>从我们的模型中获得）以及优化算法所需的超参数字典。
小批量随机梯度下降只需要设置<code class="docutils literal notranslate"><span class="pre">lr</span></code>值，这里设置为0.03。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h1><span class="section-number">3.13. </span>训练<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h1>
<p>通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。
我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。
当我们需要更复杂的模型时，高级API的优势将大大增加。
当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。</p>
<p>回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（<code class="docutils literal notranslate"><span class="pre">train_data</span></code>），
不停地从中获取一个小批量的输入和相应的标签。
对于每一个小批量，我们会进行以下步骤:</p>
<ul class="simple">
<li><p>通过调用<code class="docutils literal notranslate"><span class="pre">net(X)</span></code>生成预测并计算损失<code class="docutils literal notranslate"><span class="pre">l</span></code>（前向传播）。</p></li>
<li><p>通过进行反向传播来计算梯度。</p></li>
<li><p>通过调用优化器来更新模型参数。</p></li>
</ul>
<p>为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">clear_grad</span><span class="p">()</span>
        <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, loss </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span> <span class="p">[</span><span class="mf">0.00023696</span><span class="p">]</span>
<span class="n">epoch</span> <span class="mi">2</span><span class="p">,</span> <span class="n">loss</span> <span class="p">[</span><span class="mf">9.882164e-05</span><span class="p">]</span>
<span class="n">epoch</span> <span class="mi">3</span><span class="p">,</span> <span class="n">loss</span> <span class="p">[</span><span class="mf">9.844466e-05</span><span class="p">]</span>
</pre></div>
</div>
<p>下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。
要访问参数，我们首先从<code class="docutils literal notranslate"><span class="pre">net</span></code>访问所需的层，然后读取该层的权重和偏置。
正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w的估计误差：&#39;</span><span class="p">,</span> <span class="n">true_w</span> <span class="o">-</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_w</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b的估计误差：&#39;</span><span class="p">,</span> <span class="n">true_b</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span>w的估计误差： Tensor(shape=[2], dtype=float32, place=CPUPlace, stop_gradient=False,
       [ 0.00018191, -0.00056386])
b的估计误差： Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,
       [-0.00027847])
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h1><span class="section-number">3.14. </span>小结<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="id9">
<h1><span class="section-number">3.15. </span>练习<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h1>
<ol class="arabic">
<li><p>如果将小批量的总损失替换为小批量损失的平均值，你需要如何更改学习率？</p></li>
<li><p>查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失，即</p>
<div class="math notranslate nohighlight" id="equation-chapter-linear-networks-linear-regression-concise-0">
<span class="eqno">(3.15.1)<a class="headerlink" href="#equation-chapter-linear-networks-linear-regression-concise-0" title="Permalink to this equation">¶</a></span>\[\begin{split}l(y,y') = \begin{cases}|y-y'| -\frac{\sigma}{2} &amp; \text{ if } |y-y'| &gt; \sigma \\ \frac{1}{2 \sigma} (y-y')^2 &amp; \text{ 其它情况}\end{cases}\end{split}\]</div>
</li>
<li><p>你如何访问线性回归的梯度？</p></li>
</ol>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">3.7. 生成数据集</a></li>
<li><a class="reference internal" href="#id2">3.8. 读取数据集</a></li>
<li><a class="reference internal" href="#id3">3.9. 定义模型</a></li>
<li><a class="reference internal" href="#id4">3.10. 初始化模型参数</a></li>
<li><a class="reference internal" href="#id5">3.11. 定义损失函数</a></li>
<li><a class="reference internal" href="#id6">3.12. 定义优化算法</a></li>
<li><a class="reference internal" href="#id7">3.13. 训练</a></li>
<li><a class="reference internal" href="#id8">3.14. 小结</a></li>
<li><a class="reference internal" href="#id9">3.15. 练习</a></li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="linear-regression.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3.1. 线性回归的基本元素</div>
         </div>
     </a>
     <a id="button-next" href="softmax-regression.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3.16. softmax回归</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>