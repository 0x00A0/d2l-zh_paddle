<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>3.31. 初始化模型参数 &#8212; 动手学深度学习 2.0.0-beta0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="参考文献" href="../chapter_references/zreferences.html" />
    <link rel="prev" title="3.22. 初始化模型参数" href="softmax-regression-scratch.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">3. </span>线性神经网络</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">3.31. </span>初始化模型参数</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_linear-networks/softmax-regression-concise.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-pytorch.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh-paddle.pdf">
                  <i class="fas fa-file-pdf"></i>
                  Paddle
              </a>
          
              <a  class="mdl-navigation__link" href="https://0x00a0.github.io/d2l-zh_paddle/d2l-zh.zip">
                  <i class="fas fa-download"></i>
                  Jupyter 记事本
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai/zh-v2/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/0x00A0/d2l-zh_paddle">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">关于本书</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id11">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id12">小结</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html#id13">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装 Miniconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l">安装深度学习框架和<code class="docutils literal notranslate"><span class="pre">d2l</span></code>软件包</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html#d2l-notebook">下载 D2L Notebook</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id2">2.2. 运算符</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#subsec-broadcasting">2.3. 广播机制</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id4">2.4. 索引和切片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id5">2.5. 节省内存</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#python">2.6. 转换为其他Python对象</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id6">2.7. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html#id7">2.8. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.9. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id2">2.10. 处理缺失值</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id3">2.11. 转换为张量格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id4">2.12. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html#id5">2.13. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.14. 标量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id2">2.15. 向量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id4">2.16. 矩阵</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id5">2.17. 张量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id6">2.18. 张量算法的基本性质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subseq-lin-alg-reduction">2.19. 降维</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#dot-product">2.20. 点积（Dot Product）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id9">2.21. 矩阵-向量积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id10">2.22. 矩阵-矩阵乘法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#subsec-lin-algebra-norms">2.23. 范数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id13">2.24. 关于线性代数的更多信息</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id17">2.25. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html#id18">2.26. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.27. 导数和微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id2">2.28. 偏导数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#subsec-calculus-grad">2.29. 梯度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id4">2.30. 链式法则</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id5">2.31. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html#id6">2.32. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.33. 一个简单的例子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id2">2.34. 非标量变量的反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id3">2.35. 分离计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#python">2.36. Python控制流的梯度计算</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id4">2.37. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html#id5">2.38. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.39. 基本概率论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id4">2.40. 处理多个随机变量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id11">2.41. 期望和方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id12">2.42. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html#id13">2.43. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.44. 查找模块中的所有函数和类</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id2">2.45. 查找特定函数和类的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id3">2.46. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html#id4">2.47. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 线性神经网络</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html">3.1. 线性回归的基本元素</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id7">3.2. 矢量化加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#subsec-normal-distribution-and-squared-loss">3.3. 正态分布与平方损失</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id9">3.4. 从线性回归到深度网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id13">3.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression.html#id14">3.6. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html">3.7. 生成数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id2">3.8. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id3">3.9. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id4">3.10. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id5">3.11. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id6">3.12. 定义优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id7">3.13. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id8">3.14. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear-regression-concise.html#id9">3.15. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression.html">3.16. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html">3.17. 读取数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id4">3.18. 读取小批量</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id5">3.19. 整合所有组件</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id6">3.20. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="image-classification-dataset.html#id7">3.21. 练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html">3.22. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#softmax">3.23. 定义softmax操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id2">3.24. 定义模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id3">3.25. 定义损失函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id4">3.26. 分类精度</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id5">3.27. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id6">3.28. 预测</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id7">3.29. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax-regression-scratch.html#id8">3.30. 练习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.31. 初始化模型参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#softmax">3.32. 重新审视Softmax的实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">3.33. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">3.34. 训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">3.35. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">3.36. 练习</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <p>None # softmax回归的简洁实现</p>
<p id="sec-softmax-concise">在 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_linear_concise</span></code>中，
我们发现通过深度学习框架的高级API能够使实现</p>
<p>线性回归变得更加容易。
同样，通过深度学习框架的高级API也能更方便地实现softmax回归模型。
本节如在 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_softmax_scratch</span></code>中一样，
继续使用Fashion-MNIST数据集，并保持批量大小为256。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">paddle</span>
<span class="kn">from</span> <span class="nn">paddle</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">paddle</span> <span class="k">as</span> <span class="n">d2l</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<pre class="output literal-block">D:Anaconda3envsd2llibsite-packagespaddlefluidreader.py:355: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead
  warnings.warn(</pre>
<div class="section" id="id1">
<h1><span class="section-number">3.31. </span>初始化模型参数<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>如我们在 <a class="reference internal" href="softmax-regression.html#sec-softmax"><span class="std std-numref">3.16节</span></a>所述，
softmax回归的输出层是一个全连接层。 因此，为了实现我们的模型，
我们只需在<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>中添加一个带有10个输出的全连接层。
同样，在这里<code class="docutils literal notranslate"><span class="pre">Sequential</span></code>并不是必要的， 但它是实现深度模型的基础。
我们仍然以均值0和标准差0.01随机初始化权重。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Paddle不会隐式地调整输入的形状。因此，</span>
<span class="c1"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span>


<span class="n">weight_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">ParamAttr</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear_weight&quot;</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">weight_attr</span><span class="o">=</span><span class="n">weight_attr</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="softmax">
<span id="subsec-softmax-implementation-revisited"></span><h1><span class="section-number">3.32. </span>重新审视Softmax的实现<a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h1>
<p>在前面 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_softmax_scratch</span></code>的例子中，
我们计算了模型的输出，然后将此输出送入交叉熵损失。
从数学上讲，这是一件完全合理的事情。
然而，从计算角度来看，指数可能会造成数值稳定性问题。</p>
<p>回想一下，softmax函数<span class="math notranslate nohighlight">\(\hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}\)</span>，
其中<span class="math notranslate nohighlight">\(\hat y_j\)</span>是预测的概率分布。
<span class="math notranslate nohighlight">\(o_j\)</span>是未规范化的预测<span class="math notranslate nohighlight">\(\mathbf{o}\)</span>的第<span class="math notranslate nohighlight">\(j\)</span>个元素。
如果<span class="math notranslate nohighlight">\(o_k\)</span>中的一些数值非常大，
那么<span class="math notranslate nohighlight">\(\exp(o_k)\)</span>可能大于数据类型容许的最大数字，即<em>上溢</em>（overflow）。
这将使分母或分子变为<code class="docutils literal notranslate"><span class="pre">inf</span></code>（无穷大），
最后得到的是0、<code class="docutils literal notranslate"><span class="pre">inf</span></code>或<code class="docutils literal notranslate"><span class="pre">nan</span></code>（不是数字）的<span class="math notranslate nohighlight">\(\hat y_j\)</span>。
在这些情况下，我们无法得到一个明确定义的交叉熵值。</p>
<p>解决这个问题的一个技巧是：
在继续softmax计算之前，先从所有<span class="math notranslate nohighlight">\(o_k\)</span>中减去<span class="math notranslate nohighlight">\(\max(o_k)\)</span>。
你可以看到每个<span class="math notranslate nohighlight">\(o_k\)</span>按常数进行的移动不会改变softmax的返回值：</p>
<div class="math notranslate nohighlight" id="equation-chapter-linear-networks-softmax-regression-concise-0">
<span class="eqno">(3.32.1)<a class="headerlink" href="#equation-chapter-linear-networks-softmax-regression-concise-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\hat y_j &amp; =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
&amp; = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}\end{split}\]</div>
<p>在减法和规范化步骤之后，可能有些<span class="math notranslate nohighlight">\(o_j - \max(o_k)\)</span>具有较大的负值。
由于精度受限，<span class="math notranslate nohighlight">\(\exp(o_j - \max(o_k))\)</span>将有接近零的值，即<em>下溢</em>（underflow）。
这些值可能会四舍五入为零，使<span class="math notranslate nohighlight">\(\hat y_j\)</span>为零，
并且使得<span class="math notranslate nohighlight">\(\log(\hat y_j)\)</span>的值为<code class="docutils literal notranslate"><span class="pre">-inf</span></code>。
反向传播几步后，我们可能会发现自己面对一屏幕可怕的<code class="docutils literal notranslate"><span class="pre">nan</span></code>结果。</p>
<p>尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。
通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。
如下面的等式所示，我们避免计算<span class="math notranslate nohighlight">\(\exp(o_j - \max(o_k))\)</span>，
而可以直接使用<span class="math notranslate nohighlight">\(o_j - \max(o_k)\)</span>，因为<span class="math notranslate nohighlight">\(\log(\exp(\cdot))\)</span>被抵消了。</p>
<div class="math notranslate nohighlight" id="equation-chapter-linear-networks-softmax-regression-concise-1">
<span class="eqno">(3.32.2)<a class="headerlink" href="#equation-chapter-linear-networks-softmax-regression-concise-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\log{(\hat y_j)} &amp; = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
&amp; = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
&amp; = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
\end{aligned}\end{split}\]</div>
<p>我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。
但是，我们没有将softmax概率传递到损失函数中，
而是在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数，
这是一种类似<a class="reference external" href="https://en.wikipedia.org/wiki/LogSumExp">“LogSumExp技巧”</a>的聪明方式。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h1><span class="section-number">3.33. </span>优化算法<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h1>
<p>在这里，我们使用学习率为0.1的小批量随机梯度下降作为优化算法。
这与我们在线性回归例子中的相同，这说明了优化器的普适性。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h1><span class="section-number">3.34. </span>训练<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h1>
<p>接下来我们调用 <code class="xref std std-numref docutils literal notranslate"><span class="pre">sec_softmax_scratch</span></code>中
定义的训练函数来训练模型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---------------------------------------------------------------------------</span>

<span class="ne">AssertionError</span>                            <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>

<span class="o">~</span>\<span class="n">AppData</span>\<span class="n">Local</span>\<span class="n">Temp</span><span class="o">/</span><span class="n">ipykernel_24836</span><span class="o">/</span><span class="mf">3929993839.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
      <span class="mi">1</span> <span class="c1">#@tab all</span>
      <span class="mi">2</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="o">----&gt;</span> <span class="mi">3</span> <span class="n">d2l</span><span class="o">.</span><span class="n">train_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>


<span class="n">D</span><span class="p">:</span>\<span class="n">Anaconda3</span>\<span class="n">envs</span>\<span class="n">d2l</span>\<span class="n">lib</span>\<span class="n">site</span><span class="o">-</span><span class="n">packages</span>\<span class="n">d2l</span>\<span class="n">paddle</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="n">train_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">updater</span><span class="p">)</span>
    <span class="mi">320</span>     <span class="k">assert</span> <span class="n">train_loss</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">train_loss</span>
    <span class="mi">321</span>     <span class="k">assert</span> <span class="n">train_acc</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">train_acc</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">train_acc</span>
<span class="o">--&gt;</span> <span class="mi">322</span>     <span class="k">assert</span> <span class="n">test_acc</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">test_acc</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">test_acc</span>
    <span class="mi">323</span>
    <span class="mi">324</span> <span class="k">def</span> <span class="nf">predict_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>


<span class="ne">AssertionError</span><span class="p">:</span> <span class="mf">0.6974</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_softmax-regression-concise_1057e3_10_1.svg" src="../_images/output_softmax-regression-concise_1057e3_10_1.svg" /></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d2l</span><span class="o">.</span><span class="n">predict_ch3</span><span class="p">(</span><span class="n">net</span><span class="p">,</span><span class="n">test_iter</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_softmax-regression-concise_1057e3_11_0.svg" src="../_images/output_softmax-regression-concise_1057e3_11_0.svg" /></div>
<p>和以前一样，这个算法使结果收敛到一个相当高的精度，而且这次的代码比之前更精简了。</p>
</div>
<div class="section" id="id4">
<h1><span class="section-number">3.35. </span>小结<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。</p></li>
<li><p>从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。</p></li>
</ul>
</div>
<div class="section" id="id5">
<h1><span class="section-number">3.36. </span>练习<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h1>
<ol class="arabic simple">
<li><p>尝试调整超参数，例如批量大小、迭代周期数和学习率，并查看结果。</p></li>
<li><p>增加迭代周期的数量。为什么测试精度会在一段时间后降低？我们怎么解决这个问题？</p></li>
</ol>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">3.31. 初始化模型参数</a></li>
<li><a class="reference internal" href="#softmax">3.32. 重新审视Softmax的实现</a></li>
<li><a class="reference internal" href="#id2">3.33. 优化算法</a></li>
<li><a class="reference internal" href="#id3">3.34. 训练</a></li>
<li><a class="reference internal" href="#id4">3.35. 小结</a></li>
<li><a class="reference internal" href="#id5">3.36. 练习</a></li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="softmax-regression-scratch.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3.22. 初始化模型参数</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_references/zreferences.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>参考文献</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>